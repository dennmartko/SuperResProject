######################
###     IMPORTS    ###
######################
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Input, Dense, LeakyReLU, Conv2DTranspose, Reshape, Flatten

def GridCustomModel1(shape, data_format, C1, C2, K1, K2, D1, B1, B2, LR, momentum):
    axis = 1 if shape[0] != shape[1] else 3
    inp = Input(shape=shape)

    # Layer 3 (106, 106, 3) --> (106, 106, 7)
    lay3_1 = Conv2D(C1, (K1,K1), strides=(1,1), use_bias=B1, padding='same', data_format=data_format)(inp)
    lay3_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay3_1)
    act3 = LeakyReLU(alpha=LR)(lay3_2)
    # Layer 4 (106, 106, 64) --> (53, 53, 128)
    lay4_1 = Conv2D(4*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act3)
    lay4_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay4_1)
    act4 = LeakyReLU(alpha=LR)(lay4_2)
    # Layer 5 (53, 53, 128) --> (27, 27, 128)
    lay5_1 = Conv2D(4*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act4)
    lay5_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay5_1)
    act5 = LeakyReLU(alpha=LR)(lay5_2)
    # Layer 6 (27, 27, 128) --> (14, 14, 128)
    lay6_1 = Conv2D(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act5)
    lay6_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay6_1)
    act6 = LeakyReLU(alpha=LR)(lay6_2)
    # Layer 7 (14, 14, 128) --> (7, 7, 256)
    lay7_1 = Conv2D(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act6)
    lay7_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay7_1)
    act7 = LeakyReLU(alpha=LR)(lay7_2)
    # Layer 8 (7, 7, 256) --> (4, 4, 256)
    lay8_1 = Conv2D(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act7)
    lay8_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay8_1)
    act8 = LeakyReLU(alpha=LR)(lay8_2)
    # Layer 9 (4, 4, 256) --> (2, 2, 256)
    lay9_1 = Conv2D(D1, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act8)
    lay9_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay9_1)
    act9 = LeakyReLU(alpha=LR)(lay9_2)
    act9_flat = Flatten()(act9)

    # Dense Taking into account Global properties of the sources
    lay10_1 = Dense(2*2*D1, use_bias=False, kernel_regularizer='l1_l2')(act9_flat)
    lay10_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay10_1)
    act10 = LeakyReLU(alpha=LR)(lay10_2)
    # Reshape layer to image channels
    if shape[0] != shape[1]:
        act10_reshape = Reshape(target_shape=(D1, 2, 2))(act10)
    else:
        act10_reshape = Reshape(target_shape=(2, 2, D1))(act10)

    # Layer 11 (2, 2, 256) --> (4, 4, 256)
    lay11_1 = Conv2DTranspose(D1, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(act10_reshape)
    lay11_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay11_1)
    act11 = LeakyReLU(alpha=LR)(lay11_2)
    concat7 = tf.concat([act11, act8], axis=axis)
    # Layer 12 (4, 4, 256) --> (7, 7, 256)
    lay12_1 = Conv2DTranspose(8*C2, (4,4), strides=(2,2), use_bias=B2, padding='same', output_padding=(1,1), data_format=data_format)(concat7)
    lay12_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay12_1)
    act12 = LeakyReLU(alpha=LR)(lay12_2)
    concat6 = tf.concat([act12, act7], axis=axis)
    # Layer 13 (7, 7, 256) --> (14, 14, 128)
    lay13_1 = Conv2DTranspose(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(concat6)
    lay13_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay13_1)
    act13 = LeakyReLU(alpha=LR)(lay13_2)
    concat5 = tf.concat([act13, act6], axis=axis)
    # Layer 14 (14, 14, 128) --> (27, 27, 128)
    lay14_1 = Conv2DTranspose(8*C2, (4,4), strides=(2,2), use_bias=B2, padding='same', output_padding=(1,1), data_format=data_format)(concat5)
    lay14_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay14_1)
    act14 = LeakyReLU(alpha=LR)(lay14_2)
    concat4 = tf.concat([act14, act5], axis=axis)
    # Layer 15 (27, 27, 128) --> (53, 53, 128)
    lay15_1 = Conv2DTranspose(4*C2, (4,4), strides=(2,2), use_bias=B2, padding='same', output_padding=(1,1), data_format=data_format)(concat4)
    lay15_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay15_1)
    act15 = LeakyReLU(alpha=LR)(lay15_2)
    concat3 = tf.concat([act15, act4], axis=axis)
    # Layer 16 (53, 53, 128) --> (106, 106, 64)
    lay16_1 = Conv2DTranspose(4*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(concat3)
    lay16_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay16_1)
    act16 = LeakyReLU(alpha=LR)(lay16_2)
    concat2 = tf.concat([act16, act3], axis=axis)
    # Layer 17 (106, 106, 64) --> (212, 212, 32)
    lay17_1 = Conv2DTranspose(2*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format=data_format)(concat2)
    lay17_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay17_1)
    act17 = LeakyReLU(alpha=LR)(lay17_2)
    concat1 = tf.concat([act17, act3], axis=axis)
    # Layer 18 (212, 212, 32) --> (424, 424, 1)
    lay18_1 = Conv2DTranspose(1, (K2,K2), strides=(2,2), use_bias=B1, padding='same', data_format=data_format)(concat1)
    act18 = tf.keras.layers.Activation(activation='sigmoid', dtype="float32")(lay18_1)

    # Model
    build = tf.keras.Model(inp, act18)
    return build

def CustomModel1(shape, data_format):
    axis = 1 if shape[0] != shape[1] else 3
    inp = Input(shape=shape)
    # Layer 3 (106, 106, 3) --> (106, 106, 7)
    lay3_1 = Conv2D(64, (3,3), strides=(1,1), use_bias=False, padding='same', data_format=data_format)(inp)
    lay3_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay3_1)
    act3 = LeakyReLU(alpha=0.2)(lay3_2)
    # Layer 4 (106, 106, 64) --> (53, 53, 128)
    lay4_1 = Conv2D(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act3)
    lay4_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay4_1)
    act4 = LeakyReLU(alpha=0.2)(lay4_2)
    # Layer 5 (53, 53, 128) --> (27, 27, 128)
    lay5_1 = Conv2D(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act4)
    lay5_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay5_1)
    act5 = LeakyReLU(alpha=0.2)(lay5_2)
    # Layer 6 (27, 27, 128) --> (14, 14, 128)
    lay6_1 = Conv2D(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act5)
    lay6_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay6_1)
    act6 = LeakyReLU(alpha=0.2)(lay6_2)
    # Layer 7 (14, 14, 128) --> (7, 7, 256)
    lay7_1 = Conv2D(256, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act6)
    lay7_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay7_1)
    act7 = LeakyReLU(alpha=0.2)(lay7_2)
    # Layer 8 (7, 7, 256) --> (4, 4, 256)
    lay8_1 = Conv2D(256, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act7)
    lay8_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay8_1)
    act8 = LeakyReLU(alpha=0.2)(lay8_2)
    # Layer 9 (4, 4, 256) --> (2, 2, 256)
    lay9_1 = Conv2D(512, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act8)
    lay9_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay9_1)
    act9 = LeakyReLU(alpha=0.2)(lay9_2)
    act9_flat = Flatten()(act9)

    # Dense Taking into account Global properties of the sources
    lay10_1 = Dense(2*2*512, use_bias=False, kernel_regularizer='l1_l2')(act9_flat)
    lay10_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay10_1)
    act10 = LeakyReLU(alpha=0.2)(lay10_2)
    # Reshape layer to image channels
    if shape[0] != shape[1]:
        act10_reshape = Reshape(target_shape=(512, 2, 2))(act10)
    else:
        act10_reshape = Reshape(target_shape=(2, 2, 512))(act10)

    # Layer 11 (2, 2, 256) --> (4, 4, 256)
    lay11_1 = Conv2DTranspose(512, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(act10_reshape)
    lay11_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay11_1)
    act11 = LeakyReLU(alpha=0.2)(lay11_2)
    concat7 = tf.concat([act11, act9], axis=axis)
    # Layer 12 (4, 4, 256) --> (7, 7, 256)
    lay12_1 = Conv2DTranspose(256, (4,4), strides=(2,2), use_bias=False, padding='same', output_padding=(1,1), data_format=data_format)(concat7)
    lay12_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay12_1)
    act12 = LeakyReLU(alpha=0.2)(lay12_2)
    concat6 = tf.concat([act12, act8], axis=axis)
    # Layer 13 (7, 7, 256) --> (14, 14, 128)
    lay13_1 = Conv2DTranspose(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(concat6)
    lay13_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay13_1)
    act13 = LeakyReLU(alpha=0.2)(lay13_2)
    concat5 = tf.concat([act13, act7], axis=axis)
    # Layer 14 (14, 14, 128) --> (27, 27, 128)
    lay14_1 = Conv2DTranspose(128, (4,4), strides=(2,2), use_bias=False, padding='same', output_padding=(1,1), data_format=data_format)(concat5)
    lay14_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay14_1)
    act14 = LeakyReLU(alpha=0.2)(lay14_2)
    concat4 = tf.concat([act14, act6], axis=axis)
    # Layer 15 (27, 27, 128) --> (53, 53, 128)
    lay15_1 = Conv2DTranspose(128, (4,4), strides=(2,2), use_bias=False, padding='same', output_padding=(1,1), data_format=data_format)(concat4)
    lay15_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay15_1)
    act15 = LeakyReLU(alpha=0.2)(lay15_2)
    concat3 = tf.concat([act15, act5], axis=axis)
    # Layer 16 (53, 53, 128) --> (106, 106, 64)
    lay16_1 = Conv2DTranspose(64, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(concat3)
    lay16_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay16_1)
    act16 = LeakyReLU(alpha=0.2)(lay16_2)
    concat2 = tf.concat([act16, act4], axis=axis)
    # Layer 17 (106, 106, 64) --> (212, 212, 32)
    lay17_1 = Conv2DTranspose(32, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(concat2)
    lay17_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay17_1)
    act17 = LeakyReLU(alpha=0.2)(lay17_2)
    concat1 = tf.concat([act17, act3], axis=axis)
    # Layer 18 (212, 212, 32) --> (424, 424, 1)
    lay18_1 = Conv2DTranspose(1, (3,3), strides=(2,2), use_bias=False, padding='same', data_format=data_format)(concat1)
    act18 = tf.keras.layers.Activation(activation='sigmoid', dtype="float32")(lay18_1)

    # Model
    build = tf.keras.Model(inp, act18)
    return build