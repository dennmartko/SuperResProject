######################
###     IMPORTS    ###
######################
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Input, Dense, LeakyReLU, Conv2DTranspose, Reshape, Flatten

def GridCustomModel1(C1, C2, K1, K2, D1, B1, B2, LR, momentum):
    inp = Input(shape=(424, 424, 3))

    # Layer 1 (424, 424, 3) --> (424, 424, 16)
    lay1 = Conv2D(C1, (K1,K1), strides=(1,1),  use_bias=B1, padding='same', data_format="channels_last")(inp)
    act1 = LeakyReLU(alpha=LR)(lay1)
    # Layer 2 (424, 424, 16) --> (212, 212, 32)
    lay2_1 = Conv2D(C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act1)
    lay2_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay2_1)
    act2 = LeakyReLU(alpha=LR)(lay2_2)
    # Layer 3 (212, 212, 32) --> (106, 106, 64)
    lay3_1 = Conv2D(2*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act2)
    lay3_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay3_1)
    act3 = LeakyReLU(alpha=LR)(lay3_2)
    # Layer 4 (106, 106, 64) --> (53, 53, 128)
    lay4_1 = Conv2D(4*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act3)
    lay4_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay4_1)
    act4 = LeakyReLU(alpha=LR)(lay4_2)
    # Layer 5 (53, 53, 128) --> (27, 27, 128)
    lay5_1 = Conv2D(4*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act4)
    lay5_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay5_1)
    act5 = LeakyReLU(alpha=LR)(lay5_2)
    # Layer 6 (27, 27, 128) --> (14, 14, 128)
    lay6_1 = Conv2D(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act5)
    lay6_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay6_1)
    act6 = LeakyReLU(alpha=LR)(lay6_2)
    # Layer 7 (14, 14, 128) --> (7, 7, 256)
    lay7_1 = Conv2D(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act6)
    lay7_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay7_1)
    act7 = LeakyReLU(alpha=LR)(lay7_2)
    # Layer 8 (7, 7, 256) --> (4, 4, 256)
    lay8_1 = Conv2D(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act7)
    lay8_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay8_1)
    act8 = LeakyReLU(alpha=LR)(lay8_2)
    # Layer 9 (4, 4, 256) --> (2, 2, 256)
    lay9_1 = Conv2D(D1, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act8)
    lay9_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay9_1)
    act9 = LeakyReLU(alpha=LR)(lay9_2)
    act9_flat = Flatten()(act9)

    # Dense Taking into account Global properties of the sources
    lay10_1 = Dense(2*2*D1, use_bias=False, kernel_regularizer='l1_l2')(act9_flat)
    lay10_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay10_1)
    act10 = LeakyReLU(alpha=LR)(lay10_2)
    # Reshape layer to image channels
    act10_reshape = Reshape(target_shape=(2, 2, D1))(act10)

    # Layer 11 (2, 2, 256) --> (4, 4, 256)
    lay11_1 = Conv2DTranspose(D1, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(act10_reshape)
    lay11_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay11_1)
    act11 = LeakyReLU(alpha=LR)(lay11_2)
    concat7 = tf.concat([act11, act8], axis=3)
    # Layer 12 (4, 4, 256) --> (7, 7, 256)
    lay12_1 = Conv2DTranspose(8*C2, (4,4), strides=(2,2), use_bias=B2, padding='same', output_padding=(1,1), data_format="channels_last")(concat7)
    lay12_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay12_1)
    act12 = LeakyReLU(alpha=LR)(lay12_2)
    concat6 = tf.concat([act12, act7], axis=3)
    # Layer 13 (7, 7, 256) --> (14, 14, 128)
    lay13_1 = Conv2DTranspose(8*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(concat6)
    lay13_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay13_1)
    act13 = LeakyReLU(alpha=LR)(lay13_2)
    concat5 = tf.concat([act13, act6], axis=3)
    # Layer 14 (14, 14, 128) --> (27, 27, 128)
    lay14_1 = Conv2DTranspose(8*C2, (4,4), strides=(2,2), use_bias=B2, padding='same', output_padding=(1,1), data_format="channels_last")(concat5)
    lay14_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay14_1)
    act14 = LeakyReLU(alpha=LR)(lay14_2)
    concat4 = tf.concat([act14, act5], axis=3)
    # Layer 15 (27, 27, 128) --> (53, 53, 128)
    lay15_1 = Conv2DTranspose(4*C2, (4,4), strides=(2,2), use_bias=B2, padding='same', output_padding=(1,1), data_format="channels_last")(concat4)
    lay15_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay15_1)
    act15 = LeakyReLU(alpha=LR)(lay15_2)
    concat3 = tf.concat([act15, act4], axis=3)
    # Layer 16 (53, 53, 128) --> (106, 106, 64)
    lay16_1 = Conv2DTranspose(4*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(concat3)
    lay16_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay16_1)
    act16 = LeakyReLU(alpha=LR)(lay16_2)
    concat2 = tf.concat([act16, act3], axis=3)
    # Layer 17 (106, 106, 64) --> (212, 212, 32)
    lay17_1 = Conv2DTranspose(2*C2, (K2,K2), strides=(2,2), use_bias=B2, padding='same', data_format="channels_last")(concat2)
    lay17_2 = BatchNormalization(momentum=momentum, epsilon=1e-4)(lay17_1)
    act17 = LeakyReLU(alpha=LR)(lay17_2)
    concat1 = tf.concat([act17, act2], axis=3)
    # Layer 18 (212, 212, 32) --> (424, 424, 1)
    lay18_1 = Conv2DTranspose(1, (K2,K2), strides=(2,2), use_bias=B1, padding='same', data_format="channels_last")(concat1)
    act18 = tf.keras.layers.Activation(activation='sigmoid', dtype="float32")(lay18_1)

    # Model
    build = tf.keras.Model(inp, act18)
    return build

def CustomModel1():
    inp = Input(shape=(424, 424, 3))

    # Layer 1 (424, 424, 3) --> (424, 424, 16)
    lay1 = Conv2D(16, (3,3), strides=(1,1),  use_bias=False, padding='same', data_format="channels_last")(inp)
    act1 = LeakyReLU(alpha=0.2)(lay1)
    # Layer 2 (424, 424, 16) --> (212, 212, 32)
    lay2_1 = Conv2D(32, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act1)
    lay2_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay2_1)
    act2 = LeakyReLU(alpha=0.2)(lay2_2)
    # Layer 3 (212, 212, 32) --> (106, 106, 64)
    lay3_1 = Conv2D(64, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act2)
    lay3_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay3_1)
    act3 = LeakyReLU(alpha=0.2)(lay3_2)
    # Layer 4 (106, 106, 64) --> (53, 53, 128)
    lay4_1 = Conv2D(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act3)
    lay4_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay4_1)
    act4 = LeakyReLU(alpha=0.2)(lay4_2)
    # Layer 5 (53, 53, 128) --> (27, 27, 128)
    lay5_1 = Conv2D(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act4)
    lay5_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay5_1)
    act5 = LeakyReLU(alpha=0.2)(lay5_2)
    # Layer 6 (27, 27, 128) --> (14, 14, 128)
    lay6_1 = Conv2D(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act5)
    lay6_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay6_1)
    act6 = LeakyReLU(alpha=0.2)(lay6_2)
    # Layer 7 (14, 14, 128) --> (7, 7, 256)
    lay7_1 = Conv2D(256, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act6)
    lay7_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay7_1)
    act7 = LeakyReLU(alpha=0.2)(lay7_2)
    # Layer 8 (7, 7, 256) --> (4, 4, 256)
    lay8_1 = Conv2D(256, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act7)
    lay8_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay8_1)
    act8 = LeakyReLU(alpha=0.2)(lay8_2)
    # Layer 9 (4, 4, 256) --> (2, 2, 256)
    lay9_1 = Conv2D(512, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act8)
    lay9_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay9_1)
    act9 = LeakyReLU(alpha=0.2)(lay9_2)
    act9_flat = Flatten()(act9)

    # Dense Taking into account Global properties of the sources
    lay10_1 = Dense(2*2*512, use_bias=False, kernel_regularizer='l1_l2')(act9_flat)
    lay10_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay10_1)
    act10 = LeakyReLU(alpha=0.2)(lay10_2)
    # Reshape layer to image channels
    act10_reshape = Reshape(target_shape=(2, 2, 512))(act10)

    # Layer 11 (2, 2, 256) --> (4, 4, 256)
    lay11_1 = Conv2DTranspose(512, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(act10_reshape)
    lay11_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay11_1)
    act11 = LeakyReLU(alpha=0.2)(lay11_2)
    concat7 = tf.concat([act11, act9], axis=3)
    # Layer 12 (4, 4, 256) --> (7, 7, 256)
    lay12_1 = Conv2DTranspose(256, (4,4), strides=(2,2), use_bias=False, padding='same', output_padding=(1,1), data_format="channels_last")(concat7)
    lay12_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay12_1)
    act12 = LeakyReLU(alpha=0.2)(lay12_2)
    concat6 = tf.concat([act12, act8], axis=3)
    # Layer 13 (7, 7, 256) --> (14, 14, 128)
    lay13_1 = Conv2DTranspose(128, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(concat6)
    lay13_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay13_1)
    act13 = LeakyReLU(alpha=0.2)(lay13_2)
    concat5 = tf.concat([act13, act7], axis=3)
    # Layer 14 (14, 14, 128) --> (27, 27, 128)
    lay14_1 = Conv2DTranspose(128, (4,4), strides=(2,2), use_bias=False, padding='same', output_padding=(1,1), data_format="channels_last")(concat5)
    lay14_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay14_1)
    act14 = LeakyReLU(alpha=0.2)(lay14_2)
    concat4 = tf.concat([act14, act6], axis=3)
    # Layer 15 (27, 27, 128) --> (53, 53, 128)
    lay15_1 = Conv2DTranspose(128, (4,4), strides=(2,2), use_bias=False, padding='same', output_padding=(1,1), data_format="channels_last")(concat4)
    lay15_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay15_1)
    act15 = LeakyReLU(alpha=0.2)(lay15_2)
    concat3 = tf.concat([act15, act5], axis=3)
    # Layer 16 (53, 53, 128) --> (106, 106, 64)
    lay16_1 = Conv2DTranspose(64, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(concat3)
    lay16_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay16_1)
    act16 = LeakyReLU(alpha=0.2)(lay16_2)
    concat2 = tf.concat([act16, act4], axis=3)
    # Layer 17 (106, 106, 64) --> (212, 212, 32)
    lay17_1 = Conv2DTranspose(32, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(concat2)
    lay17_2 = BatchNormalization(momentum=0.9, epsilon=1e-4)(lay17_1)
    act17 = LeakyReLU(alpha=0.2)(lay17_2)
    concat1 = tf.concat([act17, act3], axis=3)
    # Layer 18 (212, 212, 32) --> (424, 424, 1)
    lay18_1 = Conv2DTranspose(1, (3,3), strides=(2,2), use_bias=False, padding='same', data_format="channels_last")(concat1)
    act18 = tf.keras.layers.Activation(activation='sigmoid', dtype="float32")(lay18_1)

    # Model
    build = tf.keras.Model(inp, act18)
    return build